# bike-data-lakehouse
End-to-end Databricks Lakehouse project using the Medallion Architecture (Bronze, Silver, Gold) with CSV ingestion and Unity Catalog governance.
## **Project Overview**
This project demonstrates how to build a production-style Data Lakehouse using Databricks and the Medallion Architecture (Bronze, Silver, Gold).
Raw data is ingested, cleaned, transformed, and modeled into analytics-ready datasets using PySpark and Delta Lake, following best practices used in real-world data engineering projects.
## **Architecture Overview**
Bronze Layer ‚Äì Raw data ingestion (as-is, minimal transformation)
Silver Layer ‚Äì Data cleansing, standardization, deduplication
Gold Layer ‚Äì Business-ready dimensional and fact tables
ech Stack & Tools
## üß∞ Tech Stack & Tools
‚Ä¢ **Cloud Platform:** Databricks (Community / Standard Edition)  
‚Ä¢ **Programming Languages:** Python (PySpark), SQL  
‚Ä¢ **Data Storage Format:** Delta Lake (open-source storage layer)  
‚Ä¢ **Orchestration:** Databricks Workflows (Jobs)  
‚Ä¢ **Version Control:** GitHub (Databricks Git integration)
## ‚≠ê Key Features
‚Ä¢ Implemented an end-to-end Databricks Lakehouse using the Medallion Architecture (Bronze, Silver, Gold)  
‚Ä¢ Ingested raw source data into Delta Lake Bronze tables  
‚Ä¢ Performed data cleansing, standardization, and validation in the Silver layer  
‚Ä¢ Handled null values, invalid records, and duplicate data  
‚Ä¢ Standardized date formats and business keys for reliable joins  
‚Ä¢ Built analytics-ready Gold layer with fact and dimension tables  
‚Ä¢ Applied surrogate keys to support dimensional modeling  
‚Ä¢ Used SQL to validate data and verify transformations  
‚Ä¢ Organized code using a modular, production-style folder structure  
‚Ä¢ Integrated GitHub for version control and code management


